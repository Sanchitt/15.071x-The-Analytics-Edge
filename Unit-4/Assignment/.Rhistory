install.packages("rpart.plot")
library(rpart.plot)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method = "class", minbucket = 25)
rpr(StevensTree)
prp(StevensTree)
predictCart = predict(StevensTree, type = "class", newdata = test)
table(test$Reverse, predictCart)
(41 + 71) / (41 + 71 + 22 + 36)
library(ROCR)
predictROC = predict(StevensTree, newdata = test)
head(predictROC)
pred = prediction(predictROC[,2], test$Reverse)
perf = performance(pred, "tpr", "fpr")
plot(perf)
performance(pred, "auc")@y.values
auc = as.numeric(performance(pred, "auc")@y.values)
auc
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method = "class", minbucket = 5)
plot(StevensTree)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method = "class", minbucket = 100)
plot(StevensTree)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method = "class", minbucket = 25)
prp(StevensTree)
StevensTree = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, method = "class", minbucket = 5)
prp(StevensTree)
install.packages(randomForest)
install.packages("randomForest")
library(randomForest)
stevens = read.csv("stevens.csv")
str(stevens)
library(caTools)
set.seed(3000)
spl = sample.split(stevens$Reverse, SplitRatio = 0.7)
train = subset(stevens, spl == TRUE)
test = subset(stevens, spl == FAL
train$Reverse = as.factor(train$Reverse)
test = subset(stevens, spl == FALSE)
train$Reverse = as.factor(train$Reverse)
test$Reverse = as.factor(test$Reverse)
StevesForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize = 25, ntree = 200)
predictForest = predict(StevesForest, newdata = test)
table(test$Reverse, predictForest)
(40 + 74) / ( 40 + 74 + 19 + 37)
set.seed(100)
StevesForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize = 25, ntree = 200)
predictForest = predict(StevesForest, newdata = test)
table(test$Reverse, predictForest)
(43 + 74) / ( 43 + 74 + 19 + 34)
set.seed(200)
StevesForest = randomForest(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = train, nodesize = 25, ntree = 200)
predictForest = predict(StevesForest, newdata = test)
table(test$Reverse, predictForest)
(44 + 76) / ( 44 + 76 + 17 + 33)
install.packages("caret")
View(test)
library(caret)
install.packages("e1071")
library(e1071)
numFolds = trainControl(method = "cv", number = 10)
cpGrid = expandGrid(.cp = seq(0.1, 0.5, 0.01))
cpGrid = expand.grid(.cp = seq(0.1, 0.5, 0.01))
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
Train = subset(stevens, spl == TRUE)
Test = subset(stevens, spl == FALSE)
set.seed(3000)
spl = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, spl == TRUE)
Test = subset(stevens, spl == FALSE)
stevens = read.csv("stevens.csv")
str(stevens)
library(caTools)
set.seed(3000)
spl = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, spl == TRUE)
Test = subset(stevens, spl == FALSE)
library(caret)
library(e1071)
numFolds = trainControl(method = "cv", number = 10)
cpGrid = expand.grid(.cp = seq(0.1, 0.5, 0.01))
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
cpGrid = expand.grid(.cp = seq(0.01, 0.5, 0.01))
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
stevens = read.csv("stevens.csv")
str(stevens)
library(caTools)
set.seed(3000)
spl = sample.split(stevens$Reverse, SplitRatio = 0.7)
Train = subset(stevens, spl == TRUE)
Test = subset(stevens, spl == FALSE)
library(caret)
library(e1071)
numFolds = trainControl(method = "cv", number = 10)
cpGrid = expand.grid(.cp = seq(0.01, 0.5, 0.01))
View(Test)
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "class", cp = 0.18)
predictCV = predict(StevensTreeCV, newdata = Test, type = "class")
table(Test$Reverse, predictCV)
(59 + 64) / (59 + 64 + 29 + 18)
?train
library(e1071)
(caret)
library(caret)
library(class)
library(ggplot2)
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
Train$Reverse = as.factor(Train$Reverse)
Test$Reverse = as.factor(Test$Reverse)
numFolds = trainControl(method = "cv", number = 10)
cpGrid = expand.grid(.cp = seq(0.01, 0.5, 0.01))
train(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
StevensTreeCV = rpart(Reverse ~ Circuit + Issue + Petitioner + Respondent + LowerCourt + Unconst, data = Train, method = "class", cp = 0.19)
predictCV = predict(StevensTreeCV, newdata = Test, type = "class")
table(Test$Reverse, predictCV)
(59 + 64) / (59 + 64 + 29 + 18)
prp(StevensTreeCV)
claims = read.csv("ClaimsData.csv.zip")
claims = read.csv("ClaimsData.csv")
str(claims)
mean(claims$reimbursement2008)
table(claims$bucket2009/nrow(claims))
table(claims$bucket2009)/nrow(claims)
library(caTools)
set.seed(88)
spl = sample.split(claims$bucket2009, SplitRatio = 0.6)
train = subset(claims, spl == TRUE)
test = subset(claims, spl == FALSE)
mean(train$age)
table(train$diabetes)/nrow(claims)
table(train$diabetes)/nrow(train)
table(test$bucket2009, test$bucket2008)
(110138 + 10721 +  2774 + 1539 + 104)/nrow(test)
PenaltyMatrix = matrix(c(0, 1, 2, 3, 4, 2, 0, 1, 2, 3, 4, 2, 0, 1, 2, 6, 4, 2, 0, 3, 8, 6, 4, 2, 0), byrow = TRUE, nrow = 5)
PenaltyMatrix
sum(as.matrix(table(test$bucket2009, test$bucket2008))*PenaltyMatrix)/nrow(test)
table(1, test$bucket2008)
subset(test, bucket2009 == 1)
nrow(subset(test, bucket2009 == 1))/nrow(test)
table(test$bucket2009)
table(test$bucket2009, 1)
table(test$bucket2009, c(1)*nrow(test))
c(1)*nrow(test)
matrix[1,] = 1
matrix[1,1:nrow(test)] = 1
a = matrix
a[1,1:nrow(test)] = 1
rm(A)
rm(a)
table(test$bucket2009, test$bucket2009 == 1)
seq(1, length.out = nrow(test))
seq(1"1", length.out = nrow(test))
seq(1:1, length.out = nrow(test))
seq(1,1,0)
seq(1,1,0, length.out = 10)
seq(from = 1, to = 1, by = 0, length.out = 10)
seq(from = 1, to = 1, by = 0)
seq(from = 1, to = 1, length.out = 10)
table(test$bucket2009, seq(from = 1, to = 1, length.out = nrow(test)))
as.matrix(table(test$bucket2009, seq(from = 1, to = 1, length.out = nrow(test))))
[test$age, seq(from = 1, to = 1, length.out = nrow(test))]
test$tmp = 1
View(test)
table(test$tmp, test$bucket2009)
as.matrix(table(test$tmp, test$bucket2009))
as.matrix(table(test$tmp, test$bucket2009))*PenaltyMatrix
rm(test$tmp)
test$tmp = test$bucket2009
test$tmp = 1
table(test$bucket2009, test$tmp)
as.matrix(table(test$bucket2009, test$tmp)) * PenaltyMatrix
as.matrix(table(test$bucket2009, test$tmp)) * PenaltyMatrix[,1]
sum(as.matrix(table(test$bucket2009, test$tmp)) * PenaltyMatrix[,1])
sum(as.matrix(table(test$bucket2009, test$tmp)) * PenaltyMatrix[,1])/nrow(test)
test$tmp = 1
sum(as.matrix(table(test$bucket2009, test$tmp)) * PenaltyMatrix[,1])/nrow(test)
str(test)
claimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + reimbursement2008 + bucket2008, data = train, method = "class")
claimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + reimbursement2008 + bucket2008, data = train, method = "class", cp = 0.00005)
rpr(claimsTree)
prp(claimsTree)
predictTest = predict(claimsTree, newdata = test, type = "class")
table(test$bucket2009, predictTest)
(114141 + 16102 + 118 + 201 + 0) / nrow(test)
sum(as.matrix(table(test$bucket2009, predictTest))*PenaltyMatrix)
sum(as.matrix(table(test$bucket2009, predictTest))*PenaltyMatrix)/nrow(test)
claimsTree = rpart(bucket2009 ~ age + alzheimers + arthritis + cancer + copd + depression + diabetes + heart.failure + ihd + kidney + osteoporosis + stroke + reimbursement2008 + bucket2008, data = train, method = "class", cp = 0.00005, parms = list(loss = PenaltyMatrix))
predictTest = predict(claimsTree, newdata = test, type = "class")
table(test$bucket2009, predictTest)
(94310 + 19017 + 4645 + 640 + 0) / nrow(test)
sum(as.matrix(table(test$bucket2009, predictTest))*PenaltyMatrix)/nrow(test)
boston = read.csv("boston.csv")
str(boston)
plot(boston$LAT, boston$LON)
points(boston$CHAS ==1)
points(boston$CHAS ==1, "blue")
points(boston$CHAS ==1, col="blue", pch=19)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS ==1], boston$LAT[boston$CHAS ==1], col="blue", pch=19)
points(boston$LON[boston$TRACT == 3531], boston$LAT[boston$TRACT == 3531], col="red", pch=19)
boston = read.csv("boston.csv")
str(boston)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS ==1], boston$LAT[boston$CHAS ==1], col="blue", pch=19)
points(boston$LON[boston$TRACT == 3531], boston$LAT[boston$TRACT == 3531], col="red", pch=19)
summary(boston$n)
boston = read.csv("boston.csv")
str(boston)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$CHAS ==1], boston$LAT[boston$CHAS ==1], col="blue", pch=19)
points(boston$LON[boston$TRACT == 3531], boston$LAT[boston$TRACT == 3531], col="red", pch=19)
summary(boston$NOX)
points(boston$LON[boston$NOX >= 0.55], boston$LAT[boston$NOX >= 0.55], col="green", pch=19)
plot(boston$LON, boston$LAT)
summary(boston$MEDV)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pach=19)
latlonglm  = lm(MEDV, ~ LAT + LON, data = boston)
latlonglm  = lm(MEDV ~ LAT + LON, data = boston)
summary(latlonglm)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pach=19)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
latlonglm$fitted.values
points(boston$LON[latlonglm$fitted.values > 21.2], boston$LAT[latlonglm$fitted.values > 21.2], col="blue", pch="$")
libaray(rpart)
library(rpart)
library(rpart.rplot)
library(rpart.plot)
latlontree = rpart(MEDV ~ LAT + LON, data = boston, method = "class")
prp(latlontree)
latlontree = rpart(MEDV ~ LAT + LON, data = boston)
prp(latlontree)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues > 21.2], boston$LAT[fittedvalues > 21.2], col="blue", pch="$")
latlontree = rpart(MEDV ~ LAT + LON, data = boston, minbucket = 50)
plot(latlontree)
text(latlontree)
prp(latlontree)
plot(boston$LON, boston$LAT)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
fittedvalues = predict(latlontree)
points(boston$LON[fittedvalues > 21.2], boston$LAT[fittedvalues > 21.2], col="blue", pch="$")
plot(boston$LON, boston$LAT)
abline(-71)
abline(v=-71)
abline(v=-71.07)
plot(boston$LON, boston$LAT)
abline(v=-71.07)
abline(h=-42.21)
abline(h=42.21)
abline(h=42.17)
points(boston$LON[boston$MEDV > 21.2], boston$LAT[boston$MEDV > 21.2], col="red", pch=19)
fittedvalues = predict(latlontree)
library(caTools)
spl = sample.split(boston$MEDV, SplitRatio = 0.7)
train = subset(boston, spl == TRUE)
test = subset(boston, spl == FALSE)
lnreg = lm(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train)
summary(lnreg)
summary(lnreg)
linreg.pred = predict(linreg, newdata = test)
linreg.pred = predict(lnreg, newdata = test)
linreg.sse = sum((linreg.pred - test$MEDV)^2)
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train)
prp(tree)
tree.pred = predict(tree, newdata = test)
tree.sse = sum((tree.pred - test$MEDV)^2)
library(caret)
library(e1071)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = seg(0, 0.01))
cp.grid = expand.grid(.cp = seq(0, 0.01))
View(cp.grid)
seq(0, 0.01)
cp.grid = expand.grid(.cp = seq(0, 0.01, 10))
seq(0, 0.01, 10)
cp.grid = expand.grid(.cp = seq(0, 0.01, 0.001))
seq(0, 0.01, 0.001)
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train, method = "rpart", trainControl = tr.control, tuneGrid = cp.grid)
cp.grid = expand.grid(.cp = seq(0, 0.01, 0.001))
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train, method = "rpart", trainControl = tr.control, tuneGrid = cp.grid)
cp.grid = expand.grid(.cp = (0:10)*0.001)
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train, method = "rpart", trainControl = tr.control, tuneGrid = cp.grid)
tr.control = trainControl(method = "cv", number = 10)
cp.grid = expand.grid(.cp = (0:10)*0.001)
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train, method = "rpart", trainControl = tr.control, tuneGrid = cp.grid)
tr = train(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train, method = "rpart", trControl = tr.control, tuneGrid = cp.grid)
tr
tree = rpart(MEDV ~ LAT + LON + CRIM + ZN + INDUS + CHAS + NOX + AGE + RM + DIS + RAD + TAX + PTRATIO, data = train, cp  = 0.01)
best.tree = tr$finalModel
prp(best.tree)
best.tree.pred = pred(best.tree, newdata = test)
best.tree.sse = sum((best.tree.pred - test$MEDV) ^ 2)
best.tree.pred = pred(best.tree, newdata = test)
best.tree.pred = predict(best.tree, newdata = test)
best.tree.sse = sum((best.tree.pred - test$MEDV) ^ 2)
setwd("~/Development/learning/edx/15.071x-The-Analytics-Edge/Unit-4/Assignment")
gerber = read.csv("gerber.csv")
str(gerber)
sum(gerber$voting == 1)/nrow(gerber)
table(gerber$voting)
table(gerber$voting, gerber)
table(gerber)
table(gerber, gerber$voting)
table(gerber, gerber$voting)
sum(gerber$civicduty)
sum(gerber$hawthorne)
sum(gerber$self)
sum(gerber$neighbors)
sum(gerber$civicduty)/nrow(gerber$civicduty)
sum(gerber$civicduty)/nrow(gerber$civicduty)
sum(gerber$hawthorne)/nrow(gerber$hawthorne)
sum(gerber$civicduty)/nrow(gerber$civicduty)
sum(gerber$hawthorne)/nrow(gerber$hawthorne)
sum(gerber$self)/nrow(gerber$self)
sum(gerber$neighbors)/nrow(gerber$neighbors)
sum(gerber$civicduty)/nrow(gerber$civicduty == 1)
table(gerber$civicduty, gerber$voting)
table(gerber$neighbors, gerber$voting)
table(gerber$civicduty, gerber$voting)
12021/26197
table(gerber$hawthorne, gerber$voting)
12316/25888
table(gerber$self, gerber$voting)
13191 / 25027
table(gerber$neighbors, gerber$voting)
14438 / 23763
tapply(gerber$voting, gerber$civicduty, mean)
tapply(gerber$voting, gerber$hawthorne, mean)
tapply(gerber$voting, gerber$self, mean)
tapply(gerber$voting, gerber$neighbors, mean)
logmodel = glm(voting ~ ., data = gerber, method = binomial)
summary(logmodel)
logmodel = glm(voting ~ ., data = gerber, family = binomial)
summary(logmodel)
glm(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, family="binomial")
summary(logmodel)
logmodel = glm(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, family="binomial")
summary(logmodel)
logpred = predict(logmodel)
table(gerber$voting, logmodel >= 0.3)
table(gerber$voting, logpred >= 0.3)
108696 / (108696 + 235388)
108696 + 235388
235388 / (108696 + 235388)
logpred = predict(logmodel)
logpred = predict(logmodel, type = "response")
table(gerber$voting, logpred >= 0.3)
(51966 + 134513) / (51966 + 134513 + 100875 + 56730)
table(gerber$voting, logpred >= 0.5)
logpred = predict(logmodel, type = "response")
table(gerber$voting, logpred >= 0.5)
235388 / (235388 + 108696)
library(ROCR)
predROCR = prediction(logmodel)
predROCR = prediction(logpred, "tpr", "fpr")
predROCR = prediction(logpred, gerber$voting)
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfR)
plot(perfROCR)
perfROCR = performance(predROCR, "tpr", "fpr")@y.values
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR)
auc = performance(predROCR, "tpr", "fpr")@y.values
auc
auc = as.numeric(performance(predROCR, "tpr", "fpr")@y.values)
auc = as.numeric(performance(predROCR, "auc")@y.values)
auc
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel)
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
tapply(gerber$voting, gerber$civicduty, mean)
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel3)
CARTmodelcoly = rpart(voting ~ control, data=gerber, cp=0.0)
prp(CARTmodelcoly)
CARTmodelcsx = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(CARTmodelcsx)
?prp
prp(CARTmodelcoly, digits = 6)
0.34 - 0.296638
prp(CARTmodelcsx)
0.35/0.3
0.33-0.29
prp(CARTmodelcsx, digits = 6)
logmodelcs = glm(voting ~ control + sex, data=gerber, family="binomial")
summary(logmodelcs)
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(logmodelcs, newdata=Possibilities, type="response")
0.2908065 - 0.290456
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
summary(LogModel2)
predict(LogModel2, newdata=Possibilities, type="response")
0.2904558 - 0.290456
letters = read.csv("letters_ABPR.csv")
letters$isB = as.factor(letters$letter == "B")
library(caTools)
set.seed(1000)
spl = sample.split(letters$letter, SplitRatio = 0.5)
train = subset(letters, spl == TRUE)
test = subset(letters, spl == FALSE)
table(test$isB)
1175 / (1175 + 383)
library(rpart)
library(rpart.plot)CARTb = r
library(rpart.plot)
CARTb = rpart(isB ~ . -letter, data = train, method = "class")
predCartb = predict(CARTb, newdata = test, type = "class")
table(predCartb, test$isB)
1114 + 351 / ( 1114 + 351 + 61 + 32)
(1114 + 351) / ( 1114 + 351 + 61 + 32)
library(randomForest)
Forestb = randomForest(isB ~ . -letter, data = train)
predictForest = predict(Forestb, newdata = test)
table(predictForest, test$isB)
(1162 + 373) / (1162 + 373 + 23)
letters$letter = as.factor(letters$letter)
set.seed(1000)
spl = sample.split(letters$letter, SplitRatio = 0.5)
train = subset(letters, spl == TRUE)
test = subset(letters, spl == FALSE)
table(train)
table(test$letter)
401 / (395 + 383 + 401 + 379)
CART = rpart(letter, . -isB, data = train, method = "class")
CART = rpart(letter ~ . -isB, data = train, method = "class")
predCART = predict(CART, newdata = test)
table(predCART, test$letter)
predCART = predict(CART, newdata = test, type = "class")
table(predCART, test$letter)
(350 + 314 + 366 + 338) / nrow(test)
set.seed(1000)
Forest = randomForest(letter ~ . -isB, data = train)
predictForest = predict(Forest, newdata = test)
table(predictForest, test$letter)
(395 + 374 + 397 + 362) / nrow(test)
census = read.csv("census.csv\")
library(caTools)
census = read.csv("census.csv")
census = read.csv("census.csv")
library(caTools)
set.seed(2000)
spl = sample.split(census$over50k, SplitRatio = 0.6)
train = subset(census, spl == TRUE)
test = subset(census, spl == FALSE)
logmodel = glm(over50k ~ ., data = train, family = binomial)
summary(logmodel)
predlog = predict(logmodel, newdata = test)
predlog = predict(logmodel, newdata = test, type = "response")
table(predlog >= 0.5, test$over50k)
(9051 + 1888) / (9051 + 1888 + 662 + 1190)
table(test$over50k)
9713 + 3078
9713/12791
library(ROCR)
ROCRpred = prediction(predlog, test$over50k)
performance(ROCRpred, "auc")@y.values
library(rpart)
library(rpart.plot)
tree = rpart(over50k ~ ., data = train, method = "class")
prp(tree)
predtree = predict(tree, newdata = test, type = "class")
table(predtree, test$over50k)
(9243 + 1596) / (1596 + 9243 + 470 + 1482)
ROCRperf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRperf)
predtree = predict(tree, newdata = test)
ROCRpred = prediction(predtree[,2], test$over50k)
ROCRperf = performance(ROCRpred, "tpr", "fpr")
plot(ROCRperflibrary(rpart)
)
plot(ROCRperf)
performance(ROCRpred, "auc")@y.values
set.seed(1)
trainSmall = train[sample(nrow(train), 2000),]
set.seed(1)
library(randomForest)
set.seed(1)
forest = randomForest(over50k ~ ., data = trainSmall)
predforest = predict(forest, newdata = test)
table(predforest, test$over50k)
(9586 + 1093) / (9586 + 1093 + 1985 + 127)
vu = varUsed(forest, count = TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(forest$forest$xlevels[vusorted$ix]))
varImpPlot(forest)
set.seed(2)
numFolds = trainControl(train$over50k, number = 10)
library(caret)
library(e1071)
set.seed(2)
numFolds = trainControl(method = "cv", number = 10)
grid = expand.grid(.cp = seq(0.002, 0.1, 0.001))
train(over50k ~ ., data = train, trControl = numFolds, tuneGrid = grid)
grid = expand.grid(.cp = seq(0.002, 0.1, 0.002))
train(over50k ~ ., data = train, trControl = numFolds, tuneGrid = grid)
train(over50k ~ ., data = train, method="rpart" trControl = numFolds, tuneGrid = grid)
train(over50k ~ ., data = train, method="rpart", trControl = numFolds, tuneGrid = grid)
tree = rpart(over50k ~ ., data = train, method = "class", cp = 0.02)
prp(tree)
predtree = predict(tree, newdata = test, type = "class")
table(predtree, test$over50k)
(9243 + 1596) / (9243 + 1596 + 470 + 1482)
tree = rpart(over50k ~ ., data = train, method = "class", cp = 0.002)
prp(tree)
predtree = predict(tree, newdata = test, type = "class")
table(predtree, test$over50k)
(9178 + 1838) / (9243 + 1596 + 470 + 1482)
tree$splits
nrow(tree$splits)
